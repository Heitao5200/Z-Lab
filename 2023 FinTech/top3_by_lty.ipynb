{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600bcb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from matplotlib.pyplot import plot, show\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from matplotlib.pyplot import plot, show\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score,f1_score,accuracy_score, recall_score, precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#设置随机种子\n",
    "import random\n",
    "np.random.seed(42)  \n",
    "random.seed(42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_F2(y_true, y_pred, thes=0.5):\n",
    "    y_true = y_true.apply(lambda x:0 if x==0 else 1)\n",
    "    y_pred = y_pred.apply(lambda x:0 if x<thes else 1)\n",
    "    r = recall_score(y_true, y_pred)\n",
    "    p = precision_score(y_true, y_pred)\n",
    "    print(f'precision_score:{p}, recall_score:{r}, F2:{5*p*r/(4*p+r)}')\n",
    "    return 5*p*r/(4*p+r)\n",
    "\n",
    "def cal_mape(y_true, y_pred):\n",
    "    #y_true = y_true.apply(lambda x:0 if x==0 else 1)\n",
    "    #y_pred = y_pred.apply(lambda x:0 if x==0 else 1)\n",
    "    mape_score = mape(y_true, y_pred)\n",
    "    return mape_score\n",
    "\n",
    "#压缩数据的方法（节省内存占用） from kaggle\n",
    "def reduce_mem(df, cols):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in tqdm(cols):\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# 绘制用户浏览行为时序图\n",
    "def plot_user_act(K=5, LIST=None):\n",
    "    colors = np.array( [(0,0,1),(1,0.5,0),(1,0,0)] )\n",
    "\n",
    "    for k in range(K):\n",
    "        u = np.random.choice(LIST)\n",
    "        tmp = train_view.loc[train_view.cust_wid==u]\n",
    "        tmp['day'] = pd.to_datetime(tmp['acs_tm']).dt.day\n",
    "        tmp['hour'] = pd.to_datetime(tmp['acs_tm']).dt.hour\n",
    "        \n",
    "        plt.figure(figsize=(20,5))\n",
    "        xx = np.random.uniform(-0.3,0.3,len(tmp))\n",
    "        yy = np.random.uniform(-0.5,0.5,len(tmp))\n",
    "        plt.scatter(tmp.day.values+xx, tmp.hour.values+yy, s=25, c='g')\n",
    "        plt.ylim((0,24))\n",
    "        plt.xlim((0,30))\n",
    "        c1 = mpatches.Patch(color=colors[0], label='Click page')\n",
    "        plt.plot([0,30],[6-0.5,6-0.5],'--',color='gray')\n",
    "        plt.plot([0,30],[21+0.5,21+0.5],'--',color='gray')\n",
    "        for k in range(0,30):\n",
    "            plt.plot([k+0.5,k+0.5],[0,24],'--',color='gray')\n",
    "        for k in range(1,5):\n",
    "            plt.plot([7*k+0.5,7*k+0.5],[0,24],'--',color='black')\n",
    "        plt.legend(handles=[c1])\n",
    "        plt.xlabel('Day of August 2022',size=16)\n",
    "        plt.xticks([1,5,10,15,20,25,29],['Mon\\nAug 1st','Fri\\nAug 5th','Wed\\nAug 10th','Mon\\nAug 15th','Sat\\nAug 20th','Thr\\nAug 25th','Mon\\nAug 29th'])\n",
    "        plt.ylabel('Hour of Day',size=16)\n",
    "        plt.yticks([0,4,8,12,16,20,24],['midnight','4am','8am','noon','4pm','8pm','midnight'])\n",
    "        plt.show()\n",
    "        print('\\n\\n')\n",
    "        \n",
    "\n",
    "# view表统计特征提取\n",
    "def get_view_feats(df, view):\n",
    "    dfs = []\n",
    "    \n",
    "    tmp = view.groupby('cust_wid')['cust_wid'].agg('count')\n",
    "    tmp.name = tmp.name + '_count_view'\n",
    "    dfs.append(tmp)\n",
    "    \n",
    "    tmp = view.groupby('cust_wid')['page_id'].agg('nunique')\n",
    "    tmp.name = tmp.name + '_nunique_view'\n",
    "    dfs.append(tmp)\n",
    "    \n",
    "    #每个user有多少天存在浏览记录\n",
    "    tmp = view.groupby('cust_wid')['acs_day'].agg('nunique')\n",
    "    tmp.name = tmp.name + '_nunique_view'\n",
    "    dfs.append(tmp)\n",
    "    \n",
    "    #单日最高、平均浏览次数\n",
    "    tmp_view = view.drop_duplicates(subset=['cust_wid','acs_day'])\n",
    "    for s in ['mean', 'max', 'std']:\n",
    "        tmp = tmp_view.groupby('cust_wid')['acs_day_count'].agg(s)\n",
    "        tmp.name = tmp.name + f'_{s}_view'\n",
    "        dfs.append(tmp)\n",
    "    del tmp_view\n",
    "    \n",
    "    #时间间隔大于一小时的浏览数据，“有效浏览”统计特征\n",
    "    tmp_view = view[view.acs_timestamp_diff>3600]\n",
    "    tmp = tmp_view.groupby('cust_wid')['cust_wid'].agg('count')\n",
    "    tmp.name = tmp.name + '_count_view_great_1h'\n",
    "    dfs.append(tmp)\n",
    "    tmp = tmp_view.groupby('cust_wid')['page_id'].agg('nunique')\n",
    "    tmp.name = tmp.name + '_nunique_view_great_1h'\n",
    "    dfs.append(tmp)\n",
    "    \n",
    "    \n",
    "    #最近16天统计\n",
    "    tmp_view = view[view.acs_day>15]\n",
    "    tmp = tmp_view.groupby('cust_wid')['cust_wid'].agg('count')\n",
    "    tmp.name = tmp.name + '_count_view_resent_15day'\n",
    "    dfs.append(tmp)\n",
    "    tmp = tmp_view.groupby('cust_wid')['page_id'].agg('nunique')\n",
    "    tmp.name = tmp.name + '_nunique_view_resent_15day'\n",
    "    dfs.append(tmp)\n",
    "    \n",
    "    #时间戳统计特征\n",
    "    for s in ['mean', 'max', 'std', 'skew']:\n",
    "        tmp = view.groupby('cust_wid')['acs_timestamp_diff'].agg(s)\n",
    "        tmp.name = tmp.name + f'_{s}_view'\n",
    "        dfs.append(tmp)\n",
    "        \n",
    "        \n",
    "#     # 凌晨浏览统计\n",
    "#     tmp_view = view[view.acs_hour.isin([0,1,2,3,4,5,6,7,21,22,23])]\n",
    "#     tmp = tmp_view.groupby('cust_wid')['cust_wid'].agg('count')\n",
    "#     tmp.name = tmp.name + '_count_view_lc'\n",
    "#     dfs.append(tmp)\n",
    "    \n",
    "#     # 上午浏览统计\n",
    "#     tmp_view = view[view.acs_hour.isin([8,9,10,11])]\n",
    "#     tmp = tmp_view.groupby('cust_wid')['cust_wid'].agg('count')\n",
    "#     tmp.name = tmp.name + '_count_view_sw'\n",
    "#     dfs.append(tmp)\n",
    "    \n",
    "#     # 中午浏览统计\n",
    "#     tmp_view = view[view.acs_hour.isin([12,13,14])]\n",
    "#     tmp = tmp_view.groupby('cust_wid')['cust_wid'].agg('count')\n",
    "#     tmp.name = tmp.name + '_count_view_zw'\n",
    "#     dfs.append(tmp)\n",
    "    \n",
    "#     # 下午浏览统计\n",
    "#     tmp_view = view[view.acs_hour.isin([15,16,17,18,19,20])]\n",
    "#     tmp = tmp_view.groupby('cust_wid')['cust_wid'].agg('count')\n",
    "#     tmp.name = tmp.name + '_count_view_xw'\n",
    "#     dfs.append(tmp)\n",
    "    \n",
    "    \n",
    "    feat = pd.concat(dfs,axis=1)\n",
    "    feat = feat.reset_index()\n",
    "    df = df.merge(feat,on='cust_wid',how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# trx表统计特征提取\n",
    "def get_trx_feats(df, trx):\n",
    "    dfs = []\n",
    "    \n",
    "    tmp = trx.groupby('cust_wid')['cust_wid'].agg('count')\n",
    "    tmp.name = tmp.name + '_count_trx'\n",
    "    dfs.append(tmp)\n",
    "    \n",
    "    tmp = trx.groupby('cust_wid')['trx_cd'].agg('nunique')\n",
    "    tmp.name = tmp.name + '_nunique_trx'\n",
    "    dfs.append(tmp)\n",
    "    \n",
    "    #trx['trx_amt']填充缺失\n",
    "    for s in ['sum','mean', 'min', 'max', 'std']:\n",
    "        tmp = trx.groupby('cust_wid')['trx_amt'].agg(s)\n",
    "        tmp.name = tmp.name + f'_{s}_trx'\n",
    "        dfs.append(tmp)\n",
    "        \n",
    "    for s in ['std', 'skew']:\n",
    "        tmp = trx.groupby('cust_wid')['trx_timestamp_diff'].agg(s)\n",
    "        tmp.name = tmp.name + f'_{s}_trx'\n",
    "        dfs.append(tmp)\n",
    "        \n",
    "    trx_1 = trx[trx.trx_amt>=0].copy()\n",
    "    trx_2 = trx[trx.trx_amt<0].copy()\n",
    "    for s in ['sum']:\n",
    "        tmp = trx_1.groupby('cust_wid')['trx_amt'].agg(s)\n",
    "        tmp.name = tmp.name + f'_{s}_trx1'\n",
    "        dfs.append(tmp)\n",
    "    for s in ['sum']:\n",
    "        tmp = trx_2.groupby('cust_wid')['trx_amt'].agg(s)\n",
    "        tmp.name = tmp.name + f'_{s}_trx2'\n",
    "        dfs.append(tmp)\n",
    "    feat = pd.concat(dfs,axis=1)\n",
    "    feat = feat.reset_index()\n",
    "    df = df.merge(feat,on='cust_wid',how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 训练lgb模型\n",
    "def train_model(df_train=None, stage=1, test_mode=False, aug_train=None):\n",
    "    obj = {1:'binary', 2:'multiclass'}\n",
    "    metr = {1:'auc', 2:'multi_logloss'}\n",
    "    # obj = {1:'binary', 2:'binary'}\n",
    "    # metr = {1:'auc', 2:'auc'}\n",
    "    LABEL = 'bilabel' if stage==1 else 'new_label'\n",
    "    print(LABEL)\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': 0.03,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': obj[stage],\n",
    "        'metric': metr[stage],\n",
    "        'num_leaves': 32,\n",
    "        'verbose': -1,\n",
    "        'seed': 2222,\n",
    "        'n_jobs': -1,\n",
    "\n",
    "        'min_child_weight': 5,\n",
    "        'max_depth':6,   \n",
    "        'lambda_l1':0.2,\n",
    "        'lambda_l2':0.2,\n",
    "\n",
    "        'feature_fraction': 0.2,\n",
    "        'bagging_fraction': 0.6,\n",
    "        'bagging_freq': 5,\n",
    "    }\n",
    "    if stage==2:\n",
    "        params['num_class'] = 14\n",
    "    \n",
    "\n",
    "    fold_num = 5\n",
    "    seeds = [2222]\n",
    "    oof = np.zeros(len(df_train)) if stage==1 else np.zeros([len(df_train),14])\n",
    "  \n",
    "    importance = 0\n",
    "    pred_y = pd.DataFrame()\n",
    "    score = []\n",
    "    for seed in seeds:\n",
    "        kf = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=seed)\n",
    "        # kf = KFold(n_splits=fold_num, shuffle=True, random_state=seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_train[feats], df_train[LABEL])):\n",
    "            print('-----------', fold)\n",
    "\n",
    "            trn = df_train.loc[train_idx, :]\n",
    "            val = df_train.loc[val_idx, :]\n",
    "            trn = pd.concat([trn, aug_train]).reset_index(drop=True)\n",
    "\n",
    "            train = lgb.Dataset(trn[feats],\n",
    "                                trn[LABEL])\n",
    "            valid = lgb.Dataset(val[feats],\n",
    "                                val[LABEL])\n",
    "            model = lgb.train(params, train, valid_sets=[train,valid], \n",
    "                              valid_names=('train', 'valid'),\n",
    "                              num_boost_round=3000,#categorical_feature=cat_cols,\n",
    "                              callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)])\n",
    "            if stage==1:\n",
    "                joblib.dump(model, f'tmp/lgbm_{fold}.pkl')\n",
    "            else:\n",
    "                joblib.dump(model, f'tmp/stage2_lgbm_{fold}.pkl')\n",
    "\n",
    "            oof[val_idx] += model.predict(val[feats], num_iteration=model.best_iteration) / len(seeds)\n",
    "            if test_mode:\n",
    "                pred_y['fold_%d_seed_%d' % (fold, seed)] = model.predict(df_test[feats])\n",
    "            importance += model.feature_importance(importance_type='gain') / fold_num\n",
    "            #score.append(auc(df_train.loc[val_idx, LABEL], model.predict(val[feats])))\n",
    "    df_train['oof'] = oof\n",
    "    if test_mode:\n",
    "        df_test['oof']  = pred_y.mean(axis=1).values\n",
    "\n",
    "    #print(np.mean(score), np.std(score))\n",
    "    #print(auc(df_train[LABEL],oof))\n",
    "    if test_mode:\n",
    "        return df_train, df_test, importance\n",
    "    else:\n",
    "        return df_train, importance, oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2be97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82faf1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train      = pd.read_csv('../data/train_base.csv')\n",
    "# testa      = pd.read_csv('data/testa_base.csv')\n",
    "testb      = pd.read_csv('../data/testb_base.csv')\n",
    "\n",
    "view = pd.read_feather('../view.feather')  #APP浏览数据\n",
    "trx  = pd.read_feather('../trx.feather')   #收支交易数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([testb, train],ignore_index=True)\n",
    "cat_cols = []\n",
    "for f in ['gdr_cd','cty_cd']:\n",
    "    le = LabelEncoder()\n",
    "    df[f] = le.fit_transform(df[f])\n",
    "    cat_cols.append(f)    \n",
    "df = df.sort_values(by=['cust_wid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ee2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "view['acs_timestamp'] = view['acs_tm'].apply(lambda x:int(time.mktime(time.strptime(x, \"%Y-%m-%d %H:%M\"))))\n",
    "trx['trx_timestamp']  = trx['trx_tm'].apply(lambda x:int(time.mktime(time.strptime(x, \"%Y-%m-%d %H:%M\"))))\n",
    "view['acs_timestamp_diff'] = view.groupby('cust_wid')['acs_timestamp'].diff()\n",
    "trx['trx_timestamp_diff'] = trx.groupby('cust_wid')['trx_timestamp'].diff()\n",
    "\n",
    "view['acs_day'] = view['acs_tm'].apply(lambda x:int(x[8:10]))\n",
    "view['acs_hour']= view['acs_tm'].apply(lambda x:int(x[11:13]))\n",
    "view['acs_day_count'] = view.groupby(['cust_wid','acs_day'])['acs_day'].transform('count')\n",
    "\n",
    "view = reduce_mem(view, view.columns)\n",
    "trx = reduce_mem(trx, trx.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72882b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_view_feats(df, view)\n",
    "df = get_trx_feats(df, trx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a35cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################Word2Vec##########################################\n",
    "# testa_view = pd.read_csv('data/testa_view.csv', encoding='gbk') \n",
    "# testa_view = testa_view.sort_values(by=['cust_wid','acs_tm'])\n",
    "size_dict = {'page_id_list':64}\n",
    "for f in tqdm(['page_id_list']):\n",
    "    sentences = view.groupby('cust_wid')['page_id'].agg(list).to_list()#+testa_view.groupby('cust_wid')['page_id'].agg(list).to_list()\n",
    "    for i in tqdm(range(len(sentences))):   #将每个tagid列表 转换为字符串，存储在列表sentences中\n",
    "        sentences[i] = [str(x) for x in sentences[i]]\n",
    "    emb_size = size_dict[f]\n",
    "    #sg:训练算法：1表示skip-gram,否则CBOW，默认sg=0为CBOW算法。\n",
    "    model = Word2Vec(sentences, vector_size=emb_size, window=5, min_count=2, sg=0, hs=0, workers=1, seed=1, epochs=30)\n",
    "    emb_matrix = []\n",
    "    for seq in tqdm(sentences):\n",
    "        vec = []\n",
    "        for w in seq:\n",
    "            if w in model.wv:\n",
    "                vec.append(model.wv.get_vector(w))\n",
    "        if len(vec) > 0:\n",
    "            emb_matrix.append(np.mean(vec, axis=0))\n",
    "        else:\n",
    "            emb_matrix.append([0] * emb_size)\n",
    "    emb_matrix = np.array(emb_matrix)\n",
    "    for i in range(emb_size):\n",
    "        df[f'{f}_w2v_{i}'] = emb_matrix[:150000, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea5c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征构建---Tfidf + svd\n",
    "size_dict = {'page_id_list':16}\n",
    "TfidfVectorizer_feats = []\n",
    "for f in tqdm(['page_id_list']):\n",
    "    tfidf = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (1,5))#word、char_wb\n",
    "    # tfidf = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (1,7))#word、char_wb\n",
    "    tf = tfidf.fit_transform(view.groupby('cust_wid')['page_id'].agg(list).apply(lambda x: ' '.join([str(i) for i in x])).values)\n",
    "    decom = TruncatedSVD(n_components=size_dict[f], n_iter = 50, random_state=42)\n",
    "    decom_fea = pd.DataFrame(decom.fit_transform(tf))\n",
    "    \n",
    "    decom_fea.columns = [f+f'_tfidf_svd_{i}' for i in range(size_dict[f])]\n",
    "    \n",
    "    TfidfVectorizer_feats += [f+f'_tfidf_svd_{i}' for i in range(size_dict[f])]\n",
    "    df[[f+f'_tfidf_svd_{i}' for i in range(size_dict[f])]] = decom_fea[[f+f'_tfidf_svd_{i}' for i in range(size_dict[f])]].values\n",
    "    del decom_fea\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65924261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a5612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 'bilabel'\n",
    "df_train = df[df['label'].notna()].reset_index(drop=True)\n",
    "df_test  = df[df['label'].isna()].reset_index(drop=True)\n",
    "\n",
    "df_train['bilabel'] = df_train['label'].apply(lambda x:0 if x==0 else 1)\n",
    "feats = [f for f in df_train.columns if f not in ['cust_wid',LABEL,'label', 'oof']]\n",
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, imp = train_model(df_train=df_train, stage=1, test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看特征重要性\n",
    "feats_importance = pd.DataFrame()\n",
    "feats_importance['col_name'] = feats\n",
    "feats_importance['importance'] = imp\n",
    "print(feats_importance.sort_values('importance', ascending=False)[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa228d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16adbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2791d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174cfad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e4fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd301096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
